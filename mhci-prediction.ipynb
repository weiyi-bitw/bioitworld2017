{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHC Class I-Peptides Binding Affinity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a convolutional network + LSTM to do MHC clss I binding affinity prediction from a Benchmark data set from [Kim _et al_. 2014](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The path corresponds to where you clone the bioitworld2017 repository\n",
    "PRJ = \"/workspace/bioitworld2017\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from Bio.Alphabet import IUPAC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.stats as stats\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Keras](http://keras.io) library to build the deep learning model. It is a very straightforward wrapper around the popular tensor-based library [Theano](http://deeplearning.net/software/theano/introduction.html) and Google's [TensorFlow](https://www.tensorflow.org/). The user just need to connect the layers, the library will build the low-level parameters and operations by calling the backend libraries. Eventually the model is translated into C++ code for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import (\n",
    "    Activation, BatchNormalization, Conv1D, MaxPooling1D,\n",
    "    Dense, Dropout, Embedding, LSTM, \n",
    ")\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data set\n",
    "\n",
    "Most time we are spending when building a machine learning model is in two things:\n",
    "\n",
    "  1. Cleaning and consolidating data set\n",
    "  2. Tuning parameters\n",
    "  \n",
    "I have previously re-formatting the MHC class I benchmark data set into training-ready format. Everything is wrapped within a [`pickle`](https://docs.python.org/2/library/pickle.html) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, X, y, meta, sample_weights,AA_MAP = pickle.load(\n",
    "    open(os.path.join(PRJ, \"data/mhci-v0.2.pkl\"), \"rb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickel file contains following information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw data frame: `df`\n",
    "\n",
    "The original data frame read from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>mhc</th>\n",
       "      <th>peptide_length</th>\n",
       "      <th>cv</th>\n",
       "      <th>sequence</th>\n",
       "      <th>inequality</th>\n",
       "      <th>meas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146293</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*46:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TTYVYTLPV</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46867</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*02:19</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>MLKLRVDVF</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178717</th>\n",
       "      <td>chimpanzee</td>\n",
       "      <td>Patr-B*0101</td>\n",
       "      <td>10</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VLKPGMVVTF</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>69811.320755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40889</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*02:06</td>\n",
       "      <td>10</td>\n",
       "      <td>TBD</td>\n",
       "      <td>DLAIKQYGDI</td>\n",
       "      <td>=</td>\n",
       "      <td>46835.443038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70389</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*26:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>KLYERNTAF</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109812</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*07:02</td>\n",
       "      <td>10</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VPSIKSGNDI</td>\n",
       "      <td>=</td>\n",
       "      <td>1377.283622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152591</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*54:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VPSHISSLI</td>\n",
       "      <td>=</td>\n",
       "      <td>7455.752022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170504</th>\n",
       "      <td>macaque</td>\n",
       "      <td>Mamu-B*03</td>\n",
       "      <td>11</td>\n",
       "      <td>TBD</td>\n",
       "      <td>IRQVLFLEKIE</td>\n",
       "      <td>=</td>\n",
       "      <td>7074.450208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>mouse</td>\n",
       "      <td>H-2-Kb</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VQYSNYSFL</td>\n",
       "      <td>=</td>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89889</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*33:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>QNPTMLYNK</td>\n",
       "      <td>=</td>\n",
       "      <td>23442.899447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           species          mhc  peptide_length   cv     sequence inequality  \\\n",
       "146293       human  HLA-B*46:01               9  TBD    TTYVYTLPV          >   \n",
       "46867        human  HLA-A*02:19               9  TBD    MLKLRVDVF          >   \n",
       "178717  chimpanzee  Patr-B*0101              10  TBD   VLKPGMVVTF          >   \n",
       "40889        human  HLA-A*02:06              10  TBD   DLAIKQYGDI          =   \n",
       "70389        human  HLA-A*26:01               9  TBD    KLYERNTAF          >   \n",
       "109812       human  HLA-B*07:02              10  TBD   VPSIKSGNDI          =   \n",
       "152591       human  HLA-B*54:01               9  TBD    VPSHISSLI          =   \n",
       "170504     macaque    Mamu-B*03              11  TBD  IRQVLFLEKIE          =   \n",
       "7527         mouse       H-2-Kb               9  TBD    VQYSNYSFL          =   \n",
       "89889        human  HLA-A*33:01               9  TBD    QNPTMLYNK          =   \n",
       "\n",
       "                meas  \n",
       "146293  20000.000000  \n",
       "46867   20000.000000  \n",
       "178717  69811.320755  \n",
       "40889   46835.443038  \n",
       "70389   20000.000000  \n",
       "109812   1377.283622  \n",
       "152591   7455.752022  \n",
       "170504   7074.450208  \n",
       "7527        1.080000  \n",
       "89889   23442.899447  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data matrices: `X` and `y`\n",
    "\n",
    "The transformed `numpy.ndarray` that can directly fed to the model\n",
    "\n",
    "  * `X` : the encoded amino acid sequence\n",
    "  * `y` : a vector of size (`num_samples`) with the log10 binding affinity for each peptide to the HLA subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (151933,)\n",
      "Shape of y: (151933,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta data: `meta`\n",
    "\n",
    "A data frame that connects the matrices back to the annotation, including the randomly split training and test set label (20% of test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>mhc</th>\n",
       "      <th>peptide_length</th>\n",
       "      <th>cv</th>\n",
       "      <th>sequence</th>\n",
       "      <th>inequality</th>\n",
       "      <th>meas</th>\n",
       "      <th>log10_meas</th>\n",
       "      <th>peptide_code</th>\n",
       "      <th>sample_weights</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74528</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*31:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RQLESRLGY</td>\n",
       "      <td>=</td>\n",
       "      <td>287.937493</td>\n",
       "      <td>2.459298</td>\n",
       "      <td>[14, 13, 9, 3, 15, 14, 9, 5, 19]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84137</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*68:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>TLNHVLALK</td>\n",
       "      <td>=</td>\n",
       "      <td>4.168094</td>\n",
       "      <td>0.619938</td>\n",
       "      <td>[16, 9, 11, 6, 17, 9, 0, 9, 8]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19107</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*02:02</td>\n",
       "      <td>10</td>\n",
       "      <td>TBD</td>\n",
       "      <td>ALMIAAQVVV</td>\n",
       "      <td>=</td>\n",
       "      <td>50.395007</td>\n",
       "      <td>1.702388</td>\n",
       "      <td>[0, 9, 10, 7, 0, 0, 13, 17, 17, 17]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112831</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*18:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>ADMSKLISL</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>77500.000000</td>\n",
       "      <td>4.889302</td>\n",
       "      <td>[0, 2, 10, 15, 8, 9, 7, 15, 9]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90605</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*68:02</td>\n",
       "      <td>10</td>\n",
       "      <td>TBD</td>\n",
       "      <td>IVFGIYKDNL</td>\n",
       "      <td>=</td>\n",
       "      <td>628.070550</td>\n",
       "      <td>2.798008</td>\n",
       "      <td>[7, 17, 4, 5, 7, 19, 8, 2, 11, 9]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91965</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*69:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>ARIDARIDF</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>4.301030</td>\n",
       "      <td>[0, 14, 7, 2, 0, 14, 7, 2, 4]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109569</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*15:03</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>FMNKHILSY</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[4, 10, 11, 8, 6, 7, 9, 15, 19]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42449</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*03:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VSDLYTSMR</td>\n",
       "      <td>=</td>\n",
       "      <td>2488.845673</td>\n",
       "      <td>3.395998</td>\n",
       "      <td>[17, 15, 2, 9, 19, 16, 15, 10, 14]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86662</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*68:02</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>DEFLKVPEW</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>4.301030</td>\n",
       "      <td>[2, 3, 4, 9, 8, 17, 12, 3, 18]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35323</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*02:16</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>HQIWLALRY</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>4.301030</td>\n",
       "      <td>[6, 13, 7, 18, 9, 0, 9, 14, 19]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       species          mhc  peptide_length   cv    sequence inequality  \\\n",
       "74528    human  HLA-A*31:01               9  TBD   RQLESRLGY          =   \n",
       "84137    human  HLA-A*68:01               9  TBD   TLNHVLALK          =   \n",
       "19107    human  HLA-A*02:02              10  TBD  ALMIAAQVVV          =   \n",
       "112831   human  HLA-B*18:01               9  TBD   ADMSKLISL          >   \n",
       "90605    human  HLA-A*68:02              10  TBD  IVFGIYKDNL          =   \n",
       "91965    human  HLA-A*69:01               9  TBD   ARIDARIDF          >   \n",
       "109569   human  HLA-B*15:03               9  TBD   FMNKHILSY          <   \n",
       "42449    human  HLA-A*03:01               9  TBD   VSDLYTSMR          =   \n",
       "86662    human  HLA-A*68:02               9  TBD   DEFLKVPEW          >   \n",
       "35323    human  HLA-A*02:16               9  TBD   HQIWLALRY          >   \n",
       "\n",
       "                meas  log10_meas                         peptide_code  \\\n",
       "74528     287.937493    2.459298     [14, 13, 9, 3, 15, 14, 9, 5, 19]   \n",
       "84137       4.168094    0.619938       [16, 9, 11, 6, 17, 9, 0, 9, 8]   \n",
       "19107      50.395007    1.702388  [0, 9, 10, 7, 0, 0, 13, 17, 17, 17]   \n",
       "112831  77500.000000    4.889302       [0, 2, 10, 15, 8, 9, 7, 15, 9]   \n",
       "90605     628.070550    2.798008    [7, 17, 4, 5, 7, 19, 8, 2, 11, 9]   \n",
       "91965   20000.000000    4.301030        [0, 14, 7, 2, 0, 14, 7, 2, 4]   \n",
       "109569      1.000000    0.000000      [4, 10, 11, 8, 6, 7, 9, 15, 19]   \n",
       "42449    2488.845673    3.395998   [17, 15, 2, 9, 19, 16, 15, 10, 14]   \n",
       "86662   20000.000000    4.301030       [2, 3, 4, 9, 8, 17, 12, 3, 18]   \n",
       "35323   20000.000000    4.301030      [6, 13, 7, 18, 9, 0, 9, 14, 19]   \n",
       "\n",
       "        sample_weights    set  \n",
       "74528              1.0  train  \n",
       "84137              1.0  train  \n",
       "19107              1.0  train  \n",
       "112831             0.5  train  \n",
       "90605              1.0  train  \n",
       "91965              0.5  train  \n",
       "109569             0.5  train  \n",
       "42449              1.0  train  \n",
       "86662              0.5  train  \n",
       "35323              0.5  train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amino acid code book: `AA_MAP`\n",
    "\n",
    "The mapping between the code in `X` and the amino acid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACDEFGHIKLMNPQRSTVWYBXZJUO'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IUPAC.extended_protein.letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 20,\n",
       " 'C': 1,\n",
       " 'D': 2,\n",
       " 'E': 3,\n",
       " 'F': 4,\n",
       " 'G': 5,\n",
       " 'H': 6,\n",
       " 'I': 7,\n",
       " 'J': 23,\n",
       " 'K': 8,\n",
       " 'L': 9,\n",
       " 'M': 10,\n",
       " 'N': 11,\n",
       " 'O': 25,\n",
       " 'P': 12,\n",
       " 'Q': 13,\n",
       " 'R': 14,\n",
       " 'S': 15,\n",
       " 'T': 16,\n",
       " 'U': 24,\n",
       " 'V': 17,\n",
       " 'W': 18,\n",
       " 'X': 21,\n",
       " 'Y': 19,\n",
       " 'Z': 22}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AA_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample weights: `sample_weights`\n",
    "\n",
    "From the `meta` data we can see there are some measurements with inequalities, _e.g._ > 20,000. If our objective is to get the minimum error from the predicted binding affinities, we have to down-weight these samples. For simplicity, we down-weighted these samples' weights by half, as you also see in `meta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1. ,  0.5,  0.5, ...,  1. ,  1. ,  1. ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting and splitting the data set\n",
    "\n",
    "Let's first check how many samples we have for each HLA subtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mhc\n",
       "HLA-A*01:01     4558\n",
       "HLA-A*02:01    11920\n",
       "HLA-A*02:02     4155\n",
       "HLA-A*02:03     6302\n",
       "HLA-A*02:04        4\n",
       "HLA-A*02:05       75\n",
       "HLA-A*02:06     5627\n",
       "HLA-A*02:07       80\n",
       "HLA-A*02:10       18\n",
       "HLA-A*02:11     1085\n",
       "HLA-A*02:12     1183\n",
       "HLA-A*02:16      920\n",
       "HLA-A*02:17      346\n",
       "HLA-A*02:19     1245\n",
       "HLA-A*02:50      135\n",
       "HLA-A*03:01     7089\n",
       "HLA-A*03:02       26\n",
       "HLA-A*03:19       30\n",
       "HLA-A*11:01     6255\n",
       "HLA-A*11:02       14\n",
       "HLA-A*23:01     2508\n",
       "HLA-A*24:02     3230\n",
       "HLA-A*24:03     1176\n",
       "HLA-A*25:01      960\n",
       "HLA-A*26:01     4326\n",
       "HLA-A*26:02      643\n",
       "HLA-A*26:03      536\n",
       "HLA-A*29:02     2658\n",
       "HLA-A*30:01     2778\n",
       "HLA-A*30:02     1927\n",
       "               ...  \n",
       "HLA-B*52:01       15\n",
       "HLA-B*53:01     1603\n",
       "HLA-B*54:01     1203\n",
       "HLA-B*57:01     2776\n",
       "HLA-B*57:02       18\n",
       "HLA-B*57:03       34\n",
       "HLA-B*58:01     3121\n",
       "HLA-B*58:02       51\n",
       "HLA-B*73:01      122\n",
       "HLA-B*81:01       26\n",
       "HLA-B*83:01      338\n",
       "HLA-B27            2\n",
       "HLA-B44            5\n",
       "HLA-B51            3\n",
       "HLA-B60            5\n",
       "HLA-B7            67\n",
       "HLA-B8             1\n",
       "HLA-C*03:03      154\n",
       "HLA-C*04:01      518\n",
       "HLA-C*05:01      172\n",
       "HLA-C*06:02      381\n",
       "HLA-C*07:02      143\n",
       "HLA-C*08:02       87\n",
       "HLA-C*12:03      172\n",
       "HLA-C*14:02      229\n",
       "HLA-C*15:02      181\n",
       "HLA-Cw1            4\n",
       "HLA-Cw4            2\n",
       "HLA-E*01:01        1\n",
       "HLA-E*01:03       67\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.groupby(\"mhc\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a lot of HLA types, we don't have a lot of samples. The current state-of-art approaches such as [NetMHC](http://www.cbs.dtu.dk/services/NetMHC/) train one model for one HLA subtype. Here let's focus on the largest one: HLA-A0201. We also only use peptides that's shorter than 15 amino acids.\n",
    "\n",
    "We also want to split the data set into training and testing based on the index in the metadata. The testing set will not be used in learning weights in the network in any way. We will only use it to evaluate if our model is overfitting and select a stopping point for training.\n",
    "\n",
    "One other thing: we have to reshape the vector of `X_train` and `X_test` into a matrix. We can use the `keras.preprocessing.sequence.pad_sequences` to pad the shorter sequences with an extra code (in our case, 26, given that the original coding is from 0-25).\n",
    "\n",
    "Note that we convert the matrix to float 32 for the benefit of running it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 14\n",
    "# add one padding feature\n",
    "max_features = len(IUPAC.extended_protein.letters) + 1 \n",
    "\n",
    "train_idx = meta[(meta[\"set\"] == \"train\") & \n",
    "                 (meta[\"mhc\"] == \"HLA-A*02:01\") & \n",
    "                 (meta[\"peptide_length\"] <= max_len)].index.values\n",
    "test_idx = meta[(meta[\"set\"] == \"test\") &\n",
    "                (meta[\"mhc\"] == \"HLA-A*02:01\") & \n",
    "                (meta[\"peptide_length\"] <= max_len)].index.values\n",
    "\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len, \n",
    "                                 value=max_features-1)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len, \n",
    "                                value=max_features-1)\n",
    "\n",
    "w_train = sample_weights[train_idx].astype(np.float32)\n",
    "w_test = sample_weights[test_idx].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a deep learning model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters\n",
    "\n",
    "Let's first define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "nb_epoch = 10 # only train for 10 iteration\n",
    "\n",
    "embedding_size = 32\n",
    "num_conv_filters = 32 # number of convolutional filters\n",
    "conv_filter_size = 3 # size of the convolutional filters\n",
    "num_lstm_units = 64 # number of LSTM units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 4 layers of conv\n",
    "model.add(Embedding(max_features, embedding_size, input_length=max_len))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters=num_conv_filters,\n",
    "                 kernel_size=conv_filter_size))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(LSTM(num_lstm_units))\n",
    "model.add(Dense(1, kernel_regularizer=l1_l2(l1=1e-2, l2=1e-2),\n",
    "                activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the number of parameters and the shape of output in each layer by the `.summary()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 14, 32)            864       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 12, 32)            3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 32)            48        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 6, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 28,913\n",
      "Trainable params: 28,889\n",
      "Non-trainable params: 24\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layers\n",
    "\n",
    "The embedding layers are often used in NLP (such as this [sentimental analysis from imdb](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)). It turns the indexed input, in our case the encoded amino acid sequences, into a dense vector of fixed size, _e.g._ [4, 20] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "The parameters of the embedding layers try to capture the relationships between the encoded elements, for example, the similarities between amino acids or the similar meanings across words. Elements with high similarities will have similar vector representations. \n",
    "\n",
    "The number of parameters of an embedding layer is the `dimension_of_codebook` multiplied by the `number of embedding units`. In our case, it's \n",
    "\n",
    "$$ 26 \\text{ alphabets in the codebook} \\times 32 \\text{ embedding units} = 832 \\text{ parameters}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout layer\n",
    "\n",
    "Large number of parameters tends to overfit the training data, the number of parameters in deep neural network is gigantic. One simple technique that is offen used to prevent overfitting is to insert a [dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) layer in between fully connected layers (_e.g._ LSTM). A dropout layer randomly set a fraction `p` of input units to 0 at each update during training time: \n",
    "\n",
    "![](../figures/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers\n",
    "\n",
    "The convolutional layer has a sparse connection to the adjecent units:\n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/conv_1D_nn.png)\n",
    "\n",
    "In the figure above, lines with same color correspond to the same weight. \n",
    "\n",
    "For activation, the convolution layers use Rectified Linear Unit (ReLU), which is shown to have [better convergence](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) than `tanh` function.\n",
    "\n",
    "![](http://cs231n.github.io/assets/nn1/relu.jpeg)\n",
    "\n",
    "You can see the first convolutional layer contains \n",
    "\n",
    "$$  32 \\text{ embedding units from last layer} \\times 32 \\text{ convolutional units} \\times 3 \\text{ (convolutional filter size)} + 32 \\text{ bias} = 3104 \\text{ parameters}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization layer\n",
    "\n",
    "According to [this paper](https://arxiv.org/pdf/1502.03167v3.pdf) from Google, a batch normalization layer can reduce internal covariate shift between layers and thus reduce the convergence time and might eliminatethe need for Dropout. Each unit in batch normailzation calculates the mean and standard deviation in the mini batch and shift the input by\n",
    "\n",
    "$$ y = \\gamma x + \\beta$$\n",
    "\n",
    "Therefore, the unmber of parameter is $2 \\times 14 = 28$ in the first BatchNormalization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling layer\n",
    "\n",
    "A max pooling layer parttions input into non-overlapping rectangles and output the maximum in each subregion. It reduces the data size and thus speed up the computation. Usually the pooling size is 2, too much pooling may result in loss of too much information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM layer\n",
    "\n",
    "Long short-term memory (LSTM) layer is one of the most widely used recurrent neural network (RNN) layer. The units in the recurrent neural network forms directed cycles which allow them to exhibit temporal behavior. Therefore we often see the application of RNNs in sequential data such as [handwriting recognition](https://arxiv.org/pdf/1312.4569.pdf), [speech recognition](https://www.microsoft.com/en-us/research/publication/lstm-time-and-frequency-recurrence-for-automatic-speech-recognition/), or [sentimental analysis](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "A LSTM unit is different from other RNN unit in the sense that it has a forget gate:\n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/lstm_memorycell.png)\n",
    "\n",
    "For each of the unit at time $t$, it involves the following calculation:\n",
    "\n",
    "  * Input gate \n",
    "      $$i_t = \\sigma(W_ix_t + U_i h_{t-1} + b_i)$$\n",
    "  \n",
    "  * Candidate value for the state of memory cell \n",
    "      $$\\tilde{C}_t = tanh(W_cx_t + U_ch_{t-1} + b_c)$$\n",
    "  \n",
    "  * Activation of forget cell\n",
    "      $$f_t = \\sigma(W_fx_t + U_fh_{t-1} + b_f)$$\n",
    "  \n",
    "  * The state of memory cell \n",
    "      $$C_t = i_t \\tilde{C}_t + f_t C_{t-1}$$\n",
    "  \n",
    "  * Output gate\n",
    "      $$o_t = \\sigma(W_ox_t + U_o h_{t-1} + b_o)$$\n",
    "  \n",
    "  * Output \n",
    "      $$h_t = o_t tanh(C_t)$$\n",
    "\n",
    "The above calculations involve the following parameters:\n",
    "  * $W$ : weight vector of `input_size`\n",
    "  * $U$ : weight vector of `output_size`\n",
    "  * $b$ : a scalar of bias\n",
    "\n",
    "Therefore, for instance, the LSTM layer involves\n",
    "\n",
    "$$(64 \\text{ units of LSTM layer} + 32 \\text{ units of previous layer} + 1 \\text{ bias}) \\times 64 \\text{ units} \\times 4 (\\text{input gate, candidate state, forget, output gate}) = 24832 \\text{ parameters} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure model for training\n",
    "\n",
    "With the model structure in place, we will configure the training methods for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_absolute_error\",\n",
    "              optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose optimizer\n",
    "\n",
    "Optimizers are basically a gradient descent algorithm used for optimizing the weights during each update. Here we will use the `RMSProp` optimizer as it is suggested to be [a good choice for recurrent neural networks](https://keras.io/optimizers/#rmsprop). A list of other optimizers and their formulation can be found in [this](http://sebastianruder.com/optimizing-gradient-descent/index.html)  great blog post. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `compile`\n",
    "\n",
    "Although the name might be misleading, this function does not really start compiling the model into C++ code, instead just setting more training behavior. Here we use the categorical [crossentropy](https://en.wikipedia.org/wiki/Cross_entropy) as the target value to optimize. In each iteration (or as the deep learning guys like to call it, the `epoch`) we will also output the prediction accuracy as output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally ... we start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9524 samples, validate on 2382 samples\n",
      "Epoch 1/10\n",
      "9524/9524 [==============================] - 6s - loss: 1.1099 - val_loss: 2.9796\n",
      "Epoch 2/10\n",
      "9524/9524 [==============================] - 6s - loss: 0.9123 - val_loss: 2.9498\n",
      "Epoch 3/10\n",
      "9524/9524 [==============================] - 5s - loss: 0.8334 - val_loss: 2.7713\n",
      "Epoch 4/10\n",
      "9524/9524 [==============================] - 5s - loss: 0.7765 - val_loss: 2.6282\n",
      "Epoch 5/10\n",
      "9524/9524 [==============================] - 5s - loss: 0.7463 - val_loss: 2.4708\n",
      "Epoch 6/10\n",
      "9524/9524 [==============================] - 5s - loss: 0.7132 - val_loss: 1.9644\n",
      "Epoch 7/10\n",
      "9524/9524 [==============================] - 5s - loss: 0.6953 - val_loss: 1.4714\n",
      "Epoch 8/10\n",
      "9524/9524 [==============================] - 5s - loss: 0.6764 - val_loss: 1.2187\n",
      "Epoch 9/10\n",
      "9524/9524 [==============================] - 5s - loss: 0.6594 - val_loss: 0.8720\n",
      "Epoch 10/10\n",
      "9524/9524 [==============================] - 5s - loss: 0.6512 - val_loss: 0.8044\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    sample_weight=w_train,\n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions for new data by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of evaluating the performance of the model\n",
    "\n",
    "1. A general practice in terms of binding affinity is to use IC50 < 500 as high affinity. We can use this threshold to calculate the accuracy.\n",
    "2. We can also calculate the Spearman correlation between the observed affinity with the predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.9386\n",
      "SpearmanR: 0.8055\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(y_test > np.log10(500), y_test_pred)\n",
    "spearman = stats.spearmanr(y_test, y_test_pred)\n",
    "\n",
    "print(\"AUROC: {:.4f}\".format(auc))\n",
    "print(\"SpearmanR: {:.4f}\".format(spearman.correlation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More\n",
    "\n",
    "For other techniques for training model you may go through the following links:\n",
    "\n",
    "* [IEDB automated server benchmark](http://tools.iedb.org/auto_bench/mhci/weekly/) and [the paper](https://www.ncbi.nlm.nih.gov/pubmed/25717196)\n",
    "* [`mhcflurry`](https://github.com/hammerlab/mhcflurry) is an open-source project by [Jeff Hammerbacher's lab](http://www.hammerlab.org/). It's a great source to learn about building deep learning models for MHC binding prediction, but the code is not very straitforward ...\n",
    "\n",
    "\n",
    "* Callback functions: [`ModelCheckpoint`](https://keras.io/callbacks/#modelcheckpoint) or [`EarlyStopping`](https://keras.io/callbacks/#earlystopping) are quite useful\n",
    "* Hyperparameter search using `sklearn`'s [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) function. An example can be found in `kera`'s [example script](https://github.com/fchollet/keras/blob/master/examples/mnist_sklearn_wrapper.py).\n",
    "* Parallelization: [`mxnet`](https://github.com/dmlc/mxnet) or [Spark](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)\n",
    "* An nice overview on available deep learning library out there (more focused on python): [My Top 9 Favorite Python Deep Learning Libraries](http://www.pyimagesearch.com/2016/06/27/my-top-9-favorite-python-deep-learning-libraries/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
