{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHC Class I-Peptides Binding Affinity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a convolutional network + LSTM to do MHC clss I binding affinity prediction from a Benchmark data set from [Kim _et al_. 2014](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths\n",
    "\n",
    "First things first. We will define some paths so it will be easy to refer to files we need for building models. \n",
    "\n",
    "__NOTE__: The path setting is based on the Indy New HPC (hpclogin.ind.roche.com, webportal: http://webportal.marathon.nyhpc.roche.com:3011/). You can change the paths according to your environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the paths correspond to the Indy HPC environment\n",
    "PRJ_DIR = \"/pstore/data/data_science/year/2016/chengw13/deep-learning-examples\"\n",
    "CONDAENV_DIR = \"/pstore/data/data_science/app/conda_env/dl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the conda environment so we can use the user-specificlibraries. The only way I know how to do this within the jupyter notebook is by inserting the environment paths in `sys.path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(CONDAENV_DIR, \"lib/python2.7/site-packages\"))\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "from Bio.Alphabet import IUPAC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Keras](http://keras.io) library to build the deep learning model. It is a very straightforward wrapper around the popular tensor-based library [Theano](http://deeplearning.net/software/theano/introduction.html) and Google's [TensorFlow](https://www.tensorflow.org/). The user just need to connect the layers, the library will build the low-level parameters and operations by calling the backend libraries. Eventually the model is translated into C++ code for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import (\n",
    "    Activation, BatchNormalization, Convolution1D, \n",
    "    MaxPooling1D, Dense, Dropout, Embedding, LSTM, \n",
    ")\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l1l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, `deepsense` is a python library for processing HAR datasets. It also includes some scripts to train models. We will use the classes in the library to manipulate the [WISDM dataset](http://www.cis.fordham.edu/wisdm/dataset.php#actitracker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data set\n",
    "\n",
    "Most time we are spending when building a machine learning model is in two things:\n",
    "\n",
    "  1. Cleaning and consolidating data set\n",
    "  2. Tuning parameters\n",
    "  \n",
    "I have previously re-formatting the MHC class I benchmark data set into training-ready format. Everything is wrapped within a [`pickle`](https://docs.python.org/2/library/pickle.html) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df, X, y, meta, AA_MAP, sample_weights = pickle.load(\n",
    "    open(os.path.join(PRJ_DIR, \"data/mhci.pkl\"), \"rb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickel file contains following information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw data frame: `df`\n",
    "\n",
    "The original data frame read from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>mhc</th>\n",
       "      <th>peptide_length</th>\n",
       "      <th>cv</th>\n",
       "      <th>sequence</th>\n",
       "      <th>inequality</th>\n",
       "      <th>meas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136130</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*40:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>GSNRPWVSF</td>\n",
       "      <td>=</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39938</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*02:06</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SMELPSFGV</td>\n",
       "      <td>=</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170791</th>\n",
       "      <td>macaque</td>\n",
       "      <td>Mamu-B*08</td>\n",
       "      <td>8</td>\n",
       "      <td>TBD</td>\n",
       "      <td>NREAVNHL</td>\n",
       "      <td>=</td>\n",
       "      <td>1828.851354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121030</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*15:17</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>ETQTGMHAH</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153018</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*54:01</td>\n",
       "      <td>11</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VMNSNTLLSAW</td>\n",
       "      <td>=</td>\n",
       "      <td>41144.251167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165962</th>\n",
       "      <td>macaque</td>\n",
       "      <td>Mamu-A*07</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RQGLELTLL</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>78125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101326</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*68:02</td>\n",
       "      <td>10</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VLLGGVGLVL</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44102</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*02:12</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>HTSALSLGY</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43415</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*02:11</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>SLYSGFPSL</td>\n",
       "      <td>=</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149390</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*51:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VDFKTPGTY</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        species          mhc  peptide_length   cv     sequence inequality  \\\n",
       "136130    human  HLA-B*40:01               9  TBD    GSNRPWVSF          =   \n",
       "39938     human  HLA-A*02:06               9  TBD    SMELPSFGV          =   \n",
       "170791  macaque    Mamu-B*08               8  TBD     NREAVNHL          =   \n",
       "121030    human  HLA-B*15:17               9  TBD    ETQTGMHAH          >   \n",
       "153018    human  HLA-B*54:01              11  TBD  VMNSNTLLSAW          =   \n",
       "165962  macaque    Mamu-A*07               9  TBD    RQGLELTLL          >   \n",
       "101326    human  HLA-A*68:02              10  TBD   VLLGGVGLVL          >   \n",
       "44102     human  HLA-A*02:12               9  TBD    HTSALSLGY          >   \n",
       "43415     human  HLA-A*02:11               9  TBD    SLYSGFPSL          =   \n",
       "149390    human  HLA-B*51:01               9  TBD    VDFKTPGTY          >   \n",
       "\n",
       "                meas  \n",
       "136130  20000.000000  \n",
       "39938       6.000000  \n",
       "170791   1828.851354  \n",
       "121030  20000.000000  \n",
       "153018  41144.251167  \n",
       "165962  78125.000000  \n",
       "101326  10000.000000  \n",
       "44102   20000.000000  \n",
       "43415       1.000000  \n",
       "149390  20000.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data matrices: `X` and `y`\n",
    "\n",
    "The transformed `numpy.ndarray` that can directly fed to the model\n",
    "\n",
    "  * `X` : the sensor data of size (`num_samples`), each element in the vector is an encoded sequence mapping from `A` to `Z` to `0` to `25` in a `list` type. The sequences is of length from 8 to 30.\n",
    "  * `y` : a vector of size (`num_samples`) with the log10 binding affinity for each peptide to the HLA subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (151933,)\n",
      "Shape of y: (151933,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta data: `meta`\n",
    "\n",
    "A data frame that connects the matrices back to the annotation, including the randomly split training and test set label (20% of test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>mhc</th>\n",
       "      <th>peptide_length</th>\n",
       "      <th>cv</th>\n",
       "      <th>sequence</th>\n",
       "      <th>inequality</th>\n",
       "      <th>meas</th>\n",
       "      <th>log10_meas</th>\n",
       "      <th>peptide_code</th>\n",
       "      <th>sample_weights</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124014</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*39:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>ELKRQLADL</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>4.301030</td>\n",
       "      <td>[3, 9, 8, 14, 13, 9, 0, 2, 9]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65260</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*29:02</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>KARNIISPV</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>4.301030</td>\n",
       "      <td>[8, 0, 14, 11, 7, 7, 15, 12, 17]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143923</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*57:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>HSNSEYLMF</td>\n",
       "      <td>=</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>2.127105</td>\n",
       "      <td>[6, 15, 11, 15, 3, 19, 9, 10, 4]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45145</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>ASLTPKAQR</td>\n",
       "      <td>=</td>\n",
       "      <td>289.800000</td>\n",
       "      <td>2.462098</td>\n",
       "      <td>[0, 15, 9, 16, 12, 8, 0, 13, 14]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119503</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*27:05</td>\n",
       "      <td>10</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VGKFAKIKNT</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>69791.666667</td>\n",
       "      <td>4.843804</td>\n",
       "      <td>[17, 5, 8, 4, 0, 8, 7, 8, 11, 16]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5340</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>ATSIYTIER</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>78125.000000</td>\n",
       "      <td>4.892790</td>\n",
       "      <td>[0, 16, 15, 7, 19, 16, 7, 3, 14]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88699</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*68:02</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>PYLFWLAAI</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>69767.441860</td>\n",
       "      <td>4.843653</td>\n",
       "      <td>[12, 19, 9, 4, 18, 9, 0, 0, 7]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121520</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-B*35:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>RIYKTIKQY</td>\n",
       "      <td>=</td>\n",
       "      <td>849.000000</td>\n",
       "      <td>2.928908</td>\n",
       "      <td>[14, 7, 19, 8, 16, 7, 8, 13, 19]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*01:01</td>\n",
       "      <td>8</td>\n",
       "      <td>TBD</td>\n",
       "      <td>VGNVYVKF</td>\n",
       "      <td>=</td>\n",
       "      <td>33181.844201</td>\n",
       "      <td>4.520901</td>\n",
       "      <td>[17, 5, 11, 17, 19, 17, 8, 4]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40906</th>\n",
       "      <td>human</td>\n",
       "      <td>HLA-A*03:01</td>\n",
       "      <td>9</td>\n",
       "      <td>TBD</td>\n",
       "      <td>PLARTAKVK</td>\n",
       "      <td>=</td>\n",
       "      <td>4153.700000</td>\n",
       "      <td>3.618435</td>\n",
       "      <td>[12, 9, 0, 14, 16, 0, 8, 17, 8]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       species          mhc  peptide_length   cv    sequence inequality  \\\n",
       "124014   human  HLA-B*39:01               9  TBD   ELKRQLADL          >   \n",
       "65260    human  HLA-A*29:02               9  TBD   KARNIISPV          >   \n",
       "143923   human  HLA-B*57:01               9  TBD   HSNSEYLMF          =   \n",
       "45145    human  HLA-A*11:01               9  TBD   ASLTPKAQR          =   \n",
       "119503   human  HLA-B*27:05              10  TBD  VGKFAKIKNT          >   \n",
       "5340     human  HLA-A*02:01               9  TBD   ATSIYTIER          >   \n",
       "88699    human  HLA-A*68:02               9  TBD   PYLFWLAAI          >   \n",
       "121520   human  HLA-B*35:01               9  TBD   RIYKTIKQY          =   \n",
       "56       human  HLA-A*01:01               8  TBD    VGNVYVKF          =   \n",
       "40906    human  HLA-A*03:01               9  TBD   PLARTAKVK          =   \n",
       "\n",
       "                meas  log10_meas                       peptide_code  \\\n",
       "124014  20000.000000    4.301030      [3, 9, 8, 14, 13, 9, 0, 2, 9]   \n",
       "65260   20000.000000    4.301030   [8, 0, 14, 11, 7, 7, 15, 12, 17]   \n",
       "143923    134.000000    2.127105   [6, 15, 11, 15, 3, 19, 9, 10, 4]   \n",
       "45145     289.800000    2.462098   [0, 15, 9, 16, 12, 8, 0, 13, 14]   \n",
       "119503  69791.666667    4.843804  [17, 5, 8, 4, 0, 8, 7, 8, 11, 16]   \n",
       "5340    78125.000000    4.892790   [0, 16, 15, 7, 19, 16, 7, 3, 14]   \n",
       "88699   69767.441860    4.843653     [12, 19, 9, 4, 18, 9, 0, 0, 7]   \n",
       "121520    849.000000    2.928908   [14, 7, 19, 8, 16, 7, 8, 13, 19]   \n",
       "56      33181.844201    4.520901      [17, 5, 11, 17, 19, 17, 8, 4]   \n",
       "40906    4153.700000    3.618435    [12, 9, 0, 14, 16, 0, 8, 17, 8]   \n",
       "\n",
       "        sample_weights    set  \n",
       "124014             0.5  train  \n",
       "65260              0.5  train  \n",
       "143923             1.0  train  \n",
       "45145              1.0  train  \n",
       "119503             0.5  train  \n",
       "5340               0.5  train  \n",
       "88699              0.5  train  \n",
       "121520             1.0  train  \n",
       "56                 1.0   test  \n",
       "40906              1.0  train  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amino acid code book: `AA_MAP`\n",
    "\n",
    "The mapping between the code in `X` and the amino acid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACDEFGHIKLMNPQRSTVWYBXZJUO'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IUPAC.extended_protein.letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 20,\n",
       " 'C': 1,\n",
       " 'D': 2,\n",
       " 'E': 3,\n",
       " 'F': 4,\n",
       " 'G': 5,\n",
       " 'H': 6,\n",
       " 'I': 7,\n",
       " 'J': 23,\n",
       " 'K': 8,\n",
       " 'L': 9,\n",
       " 'M': 10,\n",
       " 'N': 11,\n",
       " 'O': 25,\n",
       " 'P': 12,\n",
       " 'Q': 13,\n",
       " 'R': 14,\n",
       " 'S': 15,\n",
       " 'T': 16,\n",
       " 'U': 24,\n",
       " 'V': 17,\n",
       " 'W': 18,\n",
       " 'X': 21,\n",
       " 'Y': 19,\n",
       " 'Z': 22}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AA_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample weights: `sample_weights`\n",
    "\n",
    "From the `meta` data we can see there are some measurements with inequalities, _e.g._ > 20,000. If our objective is to get the minimum error from the predicted binding affinities, we have to down-weight these samples. For simplicity, we down-weighted these samples' weights by half, as you also see in `meta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1. ,  0.5,  0.5, ...,  1. ,  1. ,  1. ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting and splitting the data set\n",
    "\n",
    "Let's first check how many samples we have for each HLA subtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mhc\n",
       "HLA-A*01:01     4558\n",
       "HLA-A*02:01    11920\n",
       "HLA-A*02:02     4155\n",
       "HLA-A*02:03     6302\n",
       "HLA-A*02:04        4\n",
       "HLA-A*02:05       75\n",
       "HLA-A*02:06     5627\n",
       "HLA-A*02:07       80\n",
       "HLA-A*02:10       18\n",
       "HLA-A*02:11     1085\n",
       "HLA-A*02:12     1183\n",
       "HLA-A*02:16      920\n",
       "HLA-A*02:17      346\n",
       "HLA-A*02:19     1245\n",
       "HLA-A*02:50      135\n",
       "HLA-A*03:01     7089\n",
       "HLA-A*03:02       26\n",
       "HLA-A*03:19       30\n",
       "HLA-A*11:01     6255\n",
       "HLA-A*11:02       14\n",
       "HLA-A*23:01     2508\n",
       "HLA-A*24:02     3230\n",
       "HLA-A*24:03     1176\n",
       "HLA-A*25:01      960\n",
       "HLA-A*26:01     4326\n",
       "HLA-A*26:02      643\n",
       "HLA-A*26:03      536\n",
       "HLA-A*29:02     2658\n",
       "HLA-A*30:01     2778\n",
       "HLA-A*30:02     1927\n",
       "               ...  \n",
       "HLA-B*52:01       15\n",
       "HLA-B*53:01     1603\n",
       "HLA-B*54:01     1203\n",
       "HLA-B*57:01     2776\n",
       "HLA-B*57:02       18\n",
       "HLA-B*57:03       34\n",
       "HLA-B*58:01     3121\n",
       "HLA-B*58:02       51\n",
       "HLA-B*73:01      122\n",
       "HLA-B*81:01       26\n",
       "HLA-B*83:01      338\n",
       "HLA-B27            2\n",
       "HLA-B44            5\n",
       "HLA-B51            3\n",
       "HLA-B60            5\n",
       "HLA-B7            67\n",
       "HLA-B8             1\n",
       "HLA-C*03:03      154\n",
       "HLA-C*04:01      518\n",
       "HLA-C*05:01      172\n",
       "HLA-C*06:02      381\n",
       "HLA-C*07:02      143\n",
       "HLA-C*08:02       87\n",
       "HLA-C*12:03      172\n",
       "HLA-C*14:02      229\n",
       "HLA-C*15:02      181\n",
       "HLA-Cw1            4\n",
       "HLA-Cw4            2\n",
       "HLA-E*01:01        1\n",
       "HLA-E*01:03       67\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.groupby(\"mhc\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a lot of HLA types, we don't have a lot of samples. The current state-of-art approaches such as [NetMHC](http://www.cbs.dtu.dk/services/NetMHC/) train one model for one HLA subtype. Here let's focus on the largest one: HLA-A0201. We also only use peptides that's shorter than 15 amino acids.\n",
    "\n",
    "We also want to split the data set into training and testing based on the index in the metadata. The testing set will not be used in learning weights in the network in any way. We will only use it to evaluate if our model is overfitting and select a stopping point for training.\n",
    "\n",
    "One other thing: we have to reshape the vector of `X_train` and `X_test` into a matrix. We can use the `keras.preprocessing.sequence.pad_sequences` to pad the shorter sequences with an extra code (in our case, 26, given that the original coding is from 0-25).\n",
    "\n",
    "Note that we convert the matrix to float 32 for the benefit of running it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_len = 14\n",
    "# add one padding feature\n",
    "max_features = len(IUPAC.extended_protein.letters) + 1 \n",
    "\n",
    "train_idx = meta[(meta[\"set\"] == \"train\") & \n",
    "                 (meta[\"mhc\"] == \"HLA-A*02:01\") & \n",
    "                 (meta[\"peptide_length\"] <= max_len)].index.values\n",
    "test_idx = meta[(meta[\"set\"] == \"test\") &\n",
    "                (meta[\"mhc\"] == \"HLA-A*02:01\") & \n",
    "                (meta[\"peptide_length\"] <= max_len)].index.values\n",
    "\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len, \n",
    "                                 value=max_features-1)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len, \n",
    "                                value=max_features-1)\n",
    "\n",
    "w_train = sample_weights[train_idx].astype(np.float32)\n",
    "w_test = sample_weights[test_idx].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a deep learning model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters\n",
    "\n",
    "Let's first define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "nb_epoch = 10 # only train for 10 iteration\n",
    "\n",
    "embedding_size = 32\n",
    "num_conv_filters = 32 # number of convolutional filters\n",
    "conv_filter_size = 3 # size of the convolutional filters\n",
    "num_lstm_units = 64 # number of LSTM units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 4 layers of conv\n",
    "model.add(Embedding(max_features, embedding_size, input_length=max_len))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(nb_filter=num_conv_filters,\n",
    "                        filter_length=conv_filter_size,\n",
    "                        activation=\"relu\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(LSTM(num_lstm_units))\n",
    "model.add(Dense(1, W_regularizer=l1l2(l1=1e-2, l2=1e-2),\n",
    "                activation=\"relu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the number of parameters and the shape of output in each layer by the `.summary()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 14, 32)        864         embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 14, 32)        0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 12, 32)        3104        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 12, 32)        24          convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 6, 32)         0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 64)            24832       maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             65          lstm_2[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 28889\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layers\n",
    "\n",
    "The embedding layers are often used in NLP (such as this [sentimental analysis from imdb](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)). It turns the indexed input, in our case the encoded amino acid sequences, into a dense vector of fixed size, _e.g._ [4, 20] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "The parameters of the embedding layers try to capture the relationships between the encoded elements, for example, the similarities between amino acids or the similar meanings across words. Elements with high similarities will have similar vector representations. \n",
    "\n",
    "The number of parameters of an embedding layer is the `dimension_of_codebook` multiplied by the `number of embedding units`. In our case, it's \n",
    "\n",
    "$$ 26 \\text{ alphabets in the codebook} \\times 32 \\text{ embedding units} = 832 \\text{ parameters}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout layer\n",
    "\n",
    "Large number of parameters tends to overfit the training data, the number of parameters in deep neural network is gigantic. One simple technique that is offen used to prevent overfitting is to insert a [dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) layer in between fully connected layers (_e.g._ LSTM). A dropout layer randomly set a fraction `p` of input units to 0 at each update during training time: \n",
    "\n",
    "![](../figures/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers\n",
    "\n",
    "The convolutional layer has a sparse connection to the adjecent units:\n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/conv_1D_nn.png)\n",
    "\n",
    "In the figure above, lines with same color correspond to the same weight. \n",
    "\n",
    "For activation, the convolution layers use Rectified Linear Unit (ReLU), which is shown to have [better convergence](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) than `tanh` function.\n",
    "\n",
    "![](http://cs231n.github.io/assets/nn1/relu.jpeg)\n",
    "\n",
    "You can see the first convolutional layer contains \n",
    "\n",
    "$$  32 \\text{ embedding units from last layer} \\times 32 \\text{ convolutional units} \\times 3 \\text{ (convolutional filter size)} + 32 \\text{ bias} = 3104 \\text{ parameters}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization layer\n",
    "\n",
    "According to [this paper](https://arxiv.org/pdf/1502.03167v3.pdf) from Google, a batch normalization layer can reduce internal covariate shift between layers and thus reduce the convergence time and might eliminatethe need for Dropout. Each unit in batch normailzation calculates the mean and standard deviation in the mini batch and shift the input by\n",
    "\n",
    "$$ y = \\gamma x + \\beta$$\n",
    "\n",
    "Therefore, the unmber of parameter is $2 \\times 14 = 28$ in the first BatchNormalization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling layer\n",
    "\n",
    "A max pooling layer parttions input into non-overlapping rectangles and output the maximum in each subregion. It reduces the data size and thus speed up the computation. Usually the pooling size is 2, too much pooling may result in loss of too much information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM layer\n",
    "\n",
    "Long short-term memory (LSTM) layer is one of the most widely used recurrent neural network (RNN) layer. The units in the recurrent neural network forms directed cycles which allow them to exhibit temporal behavior. Therefore we often see the application of RNNs in sequential data such as [handwriting recognition](https://arxiv.org/pdf/1312.4569.pdf), [speech recognition](https://www.microsoft.com/en-us/research/publication/lstm-time-and-frequency-recurrence-for-automatic-speech-recognition/), or [sentimental analysis](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "A LSTM unit is different from other RNN unit in the sense that it has a forget gate:\n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/lstm_memorycell.png)\n",
    "\n",
    "For each of the unit at time $t$, it involves the following calculation:\n",
    "\n",
    "  * Input gate \n",
    "      $$i_t = \\sigma(W_ix_t + U_i h_{t-1} + b_i)$$\n",
    "  \n",
    "  * Candidate value for the state of memory cell \n",
    "      $$\\tilde{C}_t = tanh(W_cx_t + U_ch_{t-1} + b_c)$$\n",
    "  \n",
    "  * Activation of forget cell\n",
    "      $$f_t = \\sigma(W_fx_t + U_fh_{t-1} + b_f)$$\n",
    "  \n",
    "  * The state of memory cell \n",
    "      $$C_t = i_t \\tilde{C}_t + f_t C_{t-1}$$\n",
    "  \n",
    "  * Output gate\n",
    "      $$o_t = \\sigma(W_ox_t + U_o h_{t-1} + b_o)$$\n",
    "  \n",
    "  * Output \n",
    "      $$h_t = o_t tanh(C_t)$$\n",
    "\n",
    "The above calculations involve the following parameters:\n",
    "  * $W$ : weight vector of `input_size`\n",
    "  * $U$ : weight vector of `output_size`\n",
    "  * $b$ : a scalar of bias\n",
    "\n",
    "Therefore, for instance, the LSTM layer involves\n",
    "\n",
    "$$(64 \\text{ units of LSTM layer} + 32 \\text{ units of previous layer} + 1 \\text{ bias}) \\times 64 \\text{ units} \\times 4 (\\text{input gate, candidate state, forget, output gate}) = 24832 \\text{ parameters} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure model for training\n",
    "\n",
    "With the model structure in place, we will configure the training methods for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_absolute_error\",\n",
    "              optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose optimizer\n",
    "\n",
    "Optimizers are basically a gradient descent algorithm used for optimizing the weights during each update. Here we will use the `RMSProp` optimizer as it is suggested to be [a good choice for recurrent neural networks](https://keras.io/optimizers/#rmsprop). A list of other optimizers and their formulation can be found in [this](http://sebastianruder.com/optimizing-gradient-descent/index.html)  great blog post. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `compile`\n",
    "\n",
    "Although the name might be misleading, this function does not really start compiling the model into C++ code, instead just setting more training behavior. Here we use the categorical [crossentropy](https://en.wikipedia.org/wiki/Cross_entropy) as the target value to optimize. In each iteration (or as the deep learning guys like to call it, the `epoch`) we will also output the prediction accuracy as output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally ... we start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9524 samples, validate on 2382 samples\n",
      "Epoch 1/10\n",
      "9524/9524 [==============================] - 8s - loss: 1.0375 - val_loss: 2.9157\n",
      "Epoch 2/10\n",
      "9524/9524 [==============================] - 8s - loss: 0.8284 - val_loss: 2.7215\n",
      "Epoch 3/10\n",
      "9524/9524 [==============================] - 8s - loss: 0.7561 - val_loss: 2.5371\n",
      "Epoch 4/10\n",
      "9524/9524 [==============================] - 8s - loss: 0.7070 - val_loss: 2.2731\n",
      "Epoch 5/10\n",
      "9524/9524 [==============================] - 8s - loss: 0.6843 - val_loss: 1.9922\n",
      "Epoch 6/10\n",
      "9524/9524 [==============================] - 8s - loss: 0.6637 - val_loss: 1.4276\n",
      "Epoch 7/10\n",
      "9524/9524 [==============================] - 8s - loss: 0.6401 - val_loss: 1.2442\n",
      "Epoch 8/10\n",
      "9524/9524 [==============================] - 8s - loss: 0.6356 - val_loss: 0.7732\n",
      "Epoch 9/10\n",
      "9524/9524 [==============================] - 8s - loss: 0.6239 - val_loss: 0.6396\n",
      "Epoch 10/10\n",
      "9524/9524 [==============================] - 8s - loss: 0.6186 - val_loss: 0.6645\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=nb_epoch,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    sample_weight=w_train,\n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions for new data by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of evaluating the performance of the model\n",
    "\n",
    "1. A general practice in terms of binding affinity is to use IC50 < 500 as high affinity. We can use this threshold to calculate the accuracy.\n",
    "2. We can also calculate the Spearman correlation between the observed affinity with the predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.9436\n",
      "SpearmanR: 0.8036\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(y_test > np.log10(500), y_test_pred)\n",
    "spearman = stats.spearmanr(y_test, y_test_pred)\n",
    "\n",
    "print(\"AUROC: {:.4f}\".format(auc))\n",
    "print(\"SpearmanR: {:.4f}\".format(spearman.correlation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More\n",
    "\n",
    "For other techniques for training model you may go through the following links:\n",
    "\n",
    "* [IEDB automated server benchmark](http://tools.iedb.org/auto_bench/mhci/weekly/) and [the paper](https://www.ncbi.nlm.nih.gov/pubmed/25717196)\n",
    "* [`mhcflurry`](https://github.com/hammerlab/mhcflurry) is an open-source project by [Jeff Hammerbacher's lab](http://www.hammerlab.org/). It's a great source to learn about building deep learning models for MHC binding prediction, but the code is not very straitforward ...\n",
    "\n",
    "\n",
    "* Callback functions: [`ModelCheckpoint`](https://keras.io/callbacks/#modelcheckpoint) or [`EarlyStopping`](https://keras.io/callbacks/#earlystopping) are quite useful\n",
    "* Hyperparameter search using `sklearn`'s [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) function. An example can be found in `kera`'s [example script](https://github.com/fchollet/keras/blob/master/examples/mnist_sklearn_wrapper.py).\n",
    "* Parallelization: [`mxnet`](https://github.com/dmlc/mxnet) or [Spark](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)\n",
    "* An nice overview on available deep learning library out there (more focused on python): [My Top 9 Favorite Python Deep Learning Libraries](http://www.pyimagesearch.com/2016/06/27/my-top-9-favorite-python-deep-learning-libraries/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
